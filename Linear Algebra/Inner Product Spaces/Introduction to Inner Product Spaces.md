Section: [[Inner Product Spaces (Root)]]

Recall the [[Norms and Dot Product#Dot Product|dot product]] (the Euclidian inner product) of vectors $u$ and $v$ as $u_1v_1+u_2v_2+\dots+u_nv_n$ 
## Definition

Let $V$ be a real vector space, an **inner product** on $V$ is a function that associates each pair $u,v\in V$ to a real number $\langle u,v\rangle\in\mathbb{R}$

For all $u,v,w\in V$ and $k\in\mathbb{R}$, it must satisfy the following properties:

- **Symmetry Axiom** - $\langle u,v\rangle=\langle v,u\rangle$
- **Additivity Axiom** - $\langle u+v,w\rangle=\langle u,w\rangle+\langle v,w\rangle$
- **Homogeneity Axiom** - $\langle ku,v\rangle=k\langle u,v\rangle$
- **Positivity Axiom** - $\langle v,v\rangle\geq0$ and $\langle v,v\rangle\iff v=0$
## Norm and Distance

We can define [[Norms and Dot Product#Euclidian Norm|norm and distance]] as:

$||v||=\sqrt{\langle v,v\rangle}$ and $d(u,v)=||u-v||$ 

From this and our [[Introduction to Inner Product Spaces#Definition|previous axioms]] we can conclude the following:

- $||v||\geq0$ and $||v||=0\iff v=0$
- $||kv||=|k|\;||v||$
- $d(u,v)=d(v,u)$ and $d(u,v)=0\iff u=v$
## Orthogonality

Recall what it means to be [[Norms and Dot Product#Orthonormality|orthogonal]] and understand that two vectors are orthogonal if their inner product is zero
## Orthogonal Complements

Let $W$ be a subspace in $V$, then the set:

$W^\bot=\{x\in V|\forall u\in W,\langle u,x\rangle=0\}$

Is the **orthogonal complement** of $W$, basically all vectors within a set are orthogonal to those in its complement

We can use [[Systems of Linear Equations]] to find the orthogonal complement of a [[Vector Spaces and Linear Combinations#Vector spaces|vector space]]

Example - Find the complement of $W=span(u,v)$ where $u=(2,-3,5,4)$ and $v=(0,1,-4,7)$

Since $\langle u,x\rangle=0$ and $\langle v,x\rangle=0$ we can find the solution space of the linear system to be $W^\bot$:

$\begin{matrix}2x_1&-3x_2&+5x_3&+4x_4&=0\\&x_2&-4x_3&+7x_4&=0\end{matrix}$

So we want to perform [[Systems of Linear Equations#Algorithmic Steps to Gauss Elimination|Gauss-Jordan elimination]] on the corresponding matrix like so:

$\begin{pmatrix}2&-3&5&4&0\\0&1&-4&7&0\end{pmatrix}\rightarrow\begin{pmatrix}1&\frac{-3}{2}&\frac52&2&0\\0&1&-4&7&0\end{pmatrix}$

$\begin{pmatrix}1&\frac{-3}{2}&\frac52&2&0\\0&1&-4&7&0\end{pmatrix}\rightarrow\begin{pmatrix}1&0&\frac52&2&0\\0&1&-4&7&0\end{pmatrix}$

$x_1=-\frac52x_3-2x_4$
$x_2=4x_3-7x_4$ 
$x_3=x_3$
$x_4=x_4$ 

$\begin{pmatrix}x_1\\x_2\\x_3\\x_4\end{pmatrix}=\begin{pmatrix}-\frac52\\4\\1\\1\end{pmatrix}x_3+\begin{pmatrix}-2\\-7\\0\\0\end{pmatrix}x_3=W^\bot$

$\therefore W^\bot=span\left(\begin{pmatrix}-\frac52\\4\\1\\1\end{pmatrix},\begin{pmatrix}-2\\-7\\0\\0\end{pmatrix}\right)$ 
## Different Inner Products

With a variety of inner product types, norms, distances and orthogonality is dependent on the inner product. Here are some popular examples
## Weighted Euclidian Inner Product

Let $w1,\dots,w_n\in\mathbb{R}$ be arbitrary **positive** numbers called **weights**. The weighted Euclidian inner product on vectors $u$ and $v$ is defined as $\langle u,v\rangle=w_1u_1v_1+\dots+w_nu_nv_n$
## Matrix Inner Product on $\mathbb{R}^n$

Let $A\in\mathbb{R}$ be an [[Inverse, Rank and Kernel#Inverse|invertible]] $n\times n$ matrix and consider vectors in $\mathbb{R}^n$ as column vectors. We can define:

$\langle u,v\rangle=Au\cdot Av=(Av)^TAu$

Example - Find the inner product generated by $A$ on $u,v$ where:

$A=\begin{pmatrix}2&1\\1&2\end{pmatrix},u=\begin{pmatrix}1\\4\end{pmatrix},v=\begin{pmatrix}0\\-2\end{pmatrix}$

$\langle u,v\rangle=\begin{pmatrix}2&1\\1&2\end{pmatrix}\begin{pmatrix}1\\4\end{pmatrix}\cdot \begin{pmatrix}2&1\\1&2\end{pmatrix}\begin{pmatrix}0\\-2\end{pmatrix}$ 

$\langle u,v\rangle=\begin{pmatrix}6\\9\end{pmatrix}\cdot \begin{pmatrix}-2\\-4\end{pmatrix}$

$\langle u,v\rangle=6\cdot-2+9\cdot-4=-48$
## Standard Inner Product on $\mathbb{P}_n$ 

Remember that $\mathbb{P}_n$ is the space of all polynomials of degree at most $n$

For vectors $p=a_0+a_1x+\dots+a_nx^n,q=b_0+b_1+\cdots+b_nx^n\in\mathbb{P}_n$, we can define:

$\langle p,q\rangle=a_0b_0+a_1b_1+\cdots+a_nb_n$

Coincidentally, this is equal to the dot product on $\mathbb{R}^{n+1}$
## Evaluation Inner Product on $\mathbb{P}_n$

Fix **distinct points** $x_0,x_1,\cdots,x_n\in\mathbb{R}$, then for vectors $p=p(x),q=q(x)\in\mathbb{P}_n$ we can define:

$\langle p,q\rangle=p(x_0)q(x_0)+p(x_1)q(x_1)+\cdots+p(x_n)q(x_n)$

We call this the **evaluation inner product** at $x_0,x_1,\cdots,x_n$
## Inner Product on the Space $C[a,b]$

Recall that $C[a,b]$ consists of all functions continuous on the interval $[a,b]$

We can define operation in $C[a,b]$ **point-wise**, meaning that $f=f(x)$ and $g=g(x)$ so $(f+g)(x)=f(x)+g(x)$ and $(kf)(x)=kf(x)$

Since any function continuous on $[a,b]$ is integrable on $[a,b]$ then:

$\langle f,g\rangle =\int^{b}_{a}f(x)g(x)dx$  

This also works on $\mathbb{P}_n$ since all polynomials are continuous functions
## Complex Inner Product

For any [[Complex Numbers|complex]] vector $v$, the **Hermitian conjugation** of $v$ or $v^\dagger$ is the conjugate transpose of $v$ 

Basically $v^\dagger=\overline{v^T}$

A complex square matrix is a **Hermitian matrix** if it is equal to its own conjugate transpose

So the complex inner product on $u,v\in\mathbb{C}^n$ is:

$\langle u,v\rangle = u^\dagger v$
## Standard Inequalities

- **Pythagorean Theorem** - $||u+v||^2=||u||^2+||v||^2$
- **Cauchy-Schwarz Inequality** - $|\langle u,v\rangle|\leq||u||\;||v||$
- **Triangle Inequality** - $||u+v||\leq||u||+||v||$
## Example Exam Question

Consider a weighted Euclidian inner product with weights $(a,b,a+b)$. $v=(1,2,3)$ and $v_2=(3,4,-2)$. Find values for $a$ and $b$ such that $v_1$ and $v_2$ are orthogonal, and $||v_1|| =\sqrt{59}$

$\langle v_1,v_2\rangle=3a+8b-6(a+b)=0=2b-3a$

$||v_1||=\sqrt{\langle v_1,v_1\rangle}=\sqrt{59}$
$\langle v_1,v_1\rangle=59=a+4b+9(a+b)=13b+10a$

Then we do some crazy simultaneous equations stuff to get $a=2$ and $b=3$



